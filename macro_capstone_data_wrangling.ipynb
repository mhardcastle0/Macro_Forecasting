{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction - Data Sourcing\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The goal of this project is to use publically available macroeconomic data to predict the aggregate level of credit card defaults. This notebook represents the first stage of the project: sourcing both the independent variables, which are the macroeconomic variables that may be useful in predicting the credit card default rate, and the dependent variable, the proportion of credit cards defaulting each quarter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the API Calls\n",
    "\n",
    "The first steps are to set up the notebook to make the requisite API requests. This involves importing the packages necessary for the requests and data processing, setting up the API, and creating a list of the variables that will be pulled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the required packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fredapi\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data is sourced from Federal Reserve Economic Data (FRED) through an API. The following sets the FRED API key and sets up the fredapi package with it. While API calls can be made to the FRED API using the requests package, the fredapi package was selected as it dramatically simplifies the process. FRED API keys can be sourced at the following site:\n",
    "\n",
    "https://research.stlouisfed.org/docs/api/fred/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "API_KEY = ''\n",
    "fred = fredapi.Fred(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several data sources will be pulled for this project. This step sets up a list of dictionaries, each of which contains information necessary to pull a particular data source. The components of each dictionary are:\n",
    "* name: The name of the dataset. This is used to provide a more readable description for the field than the alphanumeric FRED codes that are used in the API.\n",
    "* code: This is the alphanumeric string that FRED uses to identify each data series. It is used in the API calls.\n",
    "* freq: Different datasets have different frequencies, including daily, weekly, monthly, quarterly, and annually. Each unique freuency requires a different processing method to make it usable in any future model, so the frequency of the series is recorded for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = [{'name': 'Ten Year Minus Two Year Yield Diff', 'code': 'T10Y2Y', 'freq': 'd'},\n",
    "          {'name': 'Ten Year Minus Three Month Yield Diff', 'code': 'T10Y3M', 'freq': 'd'},\n",
    "          {'name': 'CPI - All Items', 'code': 'CPIAUCSL', 'freq': 'm'},\n",
    "          {'name': 'Civilian Unemployment Rate', 'code': 'UNRATE', 'freq': 'm'},\n",
    "          {'name': 'US RGDP', 'code': 'GDPC1', 'freq': 'q'},\n",
    "          {'name': '30-Year Fixed Mortgage Average', 'code': 'MORTGAGE30US', 'freq': 'w'},\n",
    "          {'name': 'US NGDP', 'code': 'GDP', 'freq': 'q'},\n",
    "          {'name': 'Effective Federal Funds Rate', 'code': 'FEDFUNDS', 'freq': 'm'},\n",
    "          {'name': 'Ten Year Treasury Constant Maturity Rate', 'code': 'DGS10', 'freq': 'd'},\n",
    "          {'name': 'ICE BofAML US High Yield Master II Option-Adjusted Spread', 'code': 'BAMLH0A0HYM2', 'freq': 'd'},\n",
    "          {'name': 'S&P/Case-Shiller U.S. National Home Price Index', 'code': 'CSUSHPINSA', 'freq': 'm'},\n",
    "          {'name': '3-Month LIBOR', 'code': 'USD3MTD156N', 'freq': 'd'},\n",
    "          {'name': '3-Month Treasury Bill Secondary Rate', 'code': 'TB3MS', 'freq': 'm'},\n",
    "          {'name': '1-Month LIBOR', 'code': 'USD1MTD156N', 'freq': 'd'},\n",
    "          {'name': 'M2 Money Stock', 'code': 'M2', 'freq': 'w'},\n",
    "          {'name': '10-Year Breakeven Inflation Rate', 'code': 'T10YIE', 'freq': 'd'},\n",
    "          {'name': 'Industrial Production Index', 'code': 'INDPRO', 'freq': 'm'},\n",
    "          {'name': 'TED Spread', 'code': 'TEDRATE', 'freq': 'd'},\n",
    "          {'name': 'Trade-Weighted US Dollar Index', 'code': 'TWEXB', 'freq': 'w'},\n",
    "          {'name': 'Federal Debt: Total Public Debt as Percent of GDP', 'code': 'GFDEGDQ188S', 'freq': 'q'},\n",
    "          {'name': '1-Year Treasury Constant Maturity Rate', 'code': 'DGS1', 'freq': 'd'},\n",
    "          {'name': 'All Employees: Total Nonfarm Payrolls', 'code': 'PAYEMS', 'freq': 'm'},\n",
    "          {'name': 'Smoothed U.S. Recession Probabilities', 'code': 'RECPROUSM156N', 'freq': 'm'},\n",
    "          {'name': 'Leading Index for the United States', 'code': 'USSLIND', 'freq': 'm'},\n",
    "          {'name': 'S&P 500', 'code': 'SP500', 'freq': 'd'},\n",
    "          {'name': 'Real Median Household Income in the United States', 'code': 'MEHOINUSA672N', 'freq': 'a'},\n",
    "          {'name': 'Moody\\'s Seasoned Baa Corporate Bond Yield', 'code': 'BAA', 'freq': 'm'},\n",
    "          {'name': 'Moody\\'s Seasoned Aaa Corporate Bond Yield', 'code': 'AAA', 'freq': 'm'},\n",
    "          {'name': 'St. Louis Fed Financial Stress Index', 'code': 'STLFSI', 'freq': 'w'},\n",
    "          {'name': 'ICE BofAML US Corporate BBB Option-Adjusted Spread', 'code': 'BAMLC0A4CBBB', 'freq': 'd'},\n",
    "          {'name': 'Crude Oil Prices: West Texas Intermediate (WTI) - Cushing, Oklahoma', 'code': 'DCOILWTICO', 'freq': 'd'},\n",
    "          {'name': '5-Year, 5-Year Forward Inflation Expectation Rate', 'code': 'T5YIFR', 'freq': 'd'},\n",
    "          {'name': '2-Year Treasury Constant Maturity Rate', 'code': 'DGS2', 'freq': 'd'},\n",
    "          {'name': 'Housing Starts: Total: New Privately Owned Housing Units Started', 'code': 'HOUST', 'freq': 'm'},\n",
    "          {'name': 'U.S. / Euro Foreign Exchange Rate', 'code': 'DEXUSEU', 'freq': 'd'},\n",
    "          {'name': 'S&P/Case-Shiller 20-City Composite Home Price Index', 'code': 'SPCS20RSA', 'freq': 'm'},\n",
    "          {'name': 'ICE BofAML US High Yield Master II Effective Yield', 'code': 'BAMLH0A0HYM2EY', 'freq': 'd'},\n",
    "          {'name': '30-Year Treasury Constant Maturity Rate', 'code': 'DGS30', 'freq': 'd'},\n",
    "          {'name': 'Moody\\'s Seasoned Baa Corporate Bond Yield Relative to Yield on 10-Year Treasury Constant Maturity', 'code': 'BAA10Y', 'freq': 'd'},\n",
    "          {'name': 'Personal Saving Rate', 'code': 'PSAVERT', 'freq': 'm'},\n",
    "          {'name': '5-Year Treasury Constant Maturity Rate', 'code': 'DGS5', 'freq': 'd'},\n",
    "          {'name': 'Gold Fixing Price 10:30 A.M. (London time) in London Bullion Market, based in U.S. Dollars', 'code': 'GOLDAMGBD228NLBM', 'freq': 'd'},\n",
    "          {'name': '10-Year Treasury Constant Maturity Minus Federal Funds Rate', 'code': 'T10YFF', 'freq': 'd'},\n",
    "          {'name': 'Federal Surplus or Deficit', 'code': 'FYFSD', 'freq': 'a'},\n",
    "          {'name': 'Median Sales Price of Houses Sold for the United States', 'code': 'MSPUS', 'freq': 'q'},\n",
    "          {'name': 'Producer Price Index by Commodity for Pulp, Paper, and Allied Products: Wood Pulp', 'code': 'WPU0911', 'freq': 'q'},\n",
    "          {'name': 'ICE BofAML US High Yield CCC or Below Option-Adjusted Spread', 'code': 'BAMLH0A3HYC', 'freq': 'd'},\n",
    "          {'name': 'Total Vehicle Sales', 'code': 'TOTALSA', 'freq': 'm'},\n",
    "          {'name': 'Initial Claims', 'code': 'ICSA', 'freq': 'w'},\n",
    "          {'name': 'Interest Rates, Discount Rate for United States', 'code': 'INTDSRUSM193N', 'freq': 'm'},\n",
    "          {'name': 'Civilian Labor Force Participation Rate', 'code': 'CIVPART', 'freq': 'm'},\n",
    "          {'name': 'Dow Jones Industrial Average', 'code': 'DJIA', 'freq': 'd'},\n",
    "          {'name': 'Real gross domestic product per capita', 'code': 'A939RX0Q048SBEA', 'freq': 'q'},\n",
    "          {'name': 'Delinquency Rate on Single-Family Residential Mortgages, Booked in Domestic Offices, All Commercial Banks', 'code': 'DRSFRMACBN', 'freq': 'q'},\n",
    "          {'name': 'Real Disposable Personal Income', 'code': 'DSPIC96', 'freq': 'm'},\n",
    "          {'name': 'Monthly Supply of Houses in the United States', 'code': 'MSACSR', 'freq': 'm'},\n",
    "          {'name': 'Employed full time: Median usual weekly real earnings: Wage and salary workers: 16 years and over', 'code': 'LES1252881600Q', 'freq': 'q'},\n",
    "          {'name': 'All Employees: Manufacturing', 'code': 'MANEMP', 'freq': 'm'},\n",
    "          {'name': 'Corporate Profits After Tax (without IVA and CCAdj)', 'code': 'CP', 'freq': 'q'},\n",
    "          {'name': 'ICE BofAML US Corporate Master Option-Adjusted Spread', 'code': 'BAMLC0A0CM', 'freq': 'd'},\n",
    "          {'name': '3-Month Treasury Constant Maturity Rate', 'code': 'DGS3MO', 'freq': 'd'},\n",
    "          {'name': 'Unemployment Rate: 20 years and over', 'code': 'LNS14000024', 'freq': 'm'},\n",
    "          {'name': 'All-Transactions House Price Index for the United States', 'code': 'USSTHPI', 'freq': 'q'},\n",
    "          {'name': '15-Year Fixed Rate Mortgage Average in the United States', 'code': 'MORTGAGE15US', 'freq': 'w'},\n",
    "          {'name': 'Commercial and Industrial Loans, All Commercial Banks', 'code': 'BUSLOANS', 'freq': 'm'},\n",
    "          {'name': 'Stock Market Capitalization to GDP for United States', 'code': 'DDDM01USA156NWDB', 'freq': 'a'},\n",
    "          {'name': 'Household Debt Service Payments as a Percent of Disposable Personal Income', 'code': 'TDSP', 'freq': 'q'},\n",
    "          {'name': 'Bank Prime Loan Rate', 'code': 'MPRIME', 'freq': 'm'},\n",
    "          {'name': 'CBOE Volatility Index: VIX', 'code': 'VIXCLS', 'freq': 'd'},\n",
    "          {'name': 'NBER based Recession Indicators for the United States from the Period following the Peak through the Trough', 'code': 'USREC', 'freq': 'm'},\n",
    "          {'name': 'ICE BofAML US High Yield Master II Total Return Index Value', 'code': 'BAMLHYH0A0HYM2TRIV', 'freq': 'd'},\n",
    "          {'name': 'ICE BofAML US High Yield CCC or Below Effective Yield', 'code': 'BAMLH0A3HYCEY', 'freq': 'd'},\n",
    "          {'name': '10-Year High Quality Market (HQM) Corporate Bond Spot Rate', 'code': 'HQMCB10YR', 'freq': 'm'},\n",
    "          {'name': 'ICE BofAML US High Yield BB Option-Adjusted Spread', 'code': 'BAMLH0A1HYBB', 'freq': 'd'},\n",
    "          {'name': '12-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar', 'code': 'USD12MD156N', 'freq': 'd'},\n",
    "          {'name': 'Gross Private Domestic Investment', 'code': 'GPDI', 'freq': 'q'},\n",
    "          {'name': 'Real Gross Private Domestic Investment', 'code': 'GPDIC1', 'freq': 'q'},\n",
    "          {'name': 'Manufacturers\\' New Orders: Durable Goods', 'code': 'DGORDER', 'freq': 'm'},\n",
    "          {'name': 'Producer Price Index for All Commodities', 'code': 'PPIACO', 'freq': 'm'},\n",
    "          {'name': 'Chicago Fed National Financial Conditions Index', 'code': 'NFCI', 'freq': 'w'},\n",
    "          {'name': 'Unemployment Rate: 20 years and over, Black or African American Men', 'code': 'LNS14000031', 'freq': 'm'},\n",
    "          {'name': 'Nonfinancial corporate business; debt securities; liability, Level', 'code': 'NCBDBIQ027S', 'freq': 'q'},\n",
    "          {'name': 'Capacity Utilization: Total Industry', 'code': 'TCU', 'freq': 'm'},\n",
    "          {'name': 'ICE BofAML US High Yield BB Effective Yield', 'code': 'BAMLH0A1HYBBEY', 'freq': 'd'},\n",
    "          {'name': 'Average Sales Price of Houses Sold for the United States', 'code': 'ASPUS', 'freq': 'q'},\n",
    "          {'name': 'Real Disposable Personal Income: Per Capita', 'code': 'A229RX0', 'freq': 'm'},\n",
    "          {'name': 'Unemployment Level', 'code': 'UNEMPLOY', 'freq': 'm'},\n",
    "          {'name': 'Homeownership Rate for the United States', 'code': 'RHORUSQ156N', 'freq': 'q'},\n",
    "          {'name': 'Motor Vehicle Retail Sales: Heavy Weight Trucks', 'code': 'HTRUCKSSAAR', 'freq': 'm'},\n",
    "          {'name': 'Civilian Labor Force Participation Rate: 25 to 54 years', 'code': 'LNS11300060', 'freq': 'm'},\n",
    "          {'name': 'University of Michigan: Consumer Sentiment', 'code': 'UMCSENT', 'freq': 'm'},\n",
    "          {'name': 'Consumer Opinion Surveys: Confidence Indicators: Composite Indicators: OECD Indicator for the United States', 'code': 'CSCICP03USM665S', 'freq': 'm'},\n",
    "          {'name': 'Business Tendency Surveys for Manufacturing: Confidence Indicators: Composite Indicators: OECD Indicator for the United States', 'code': 'BSCICP03USM665S', 'freq': 'm'},\n",
    "          {'name': 'Sahm Real-Time Recession Indicator', 'code': 'SAHMREALTIME', 'freq': 'm'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions for Data Manipulation\n",
    "\n",
    "Each of the datasets defined above will be be pulled from the API and manipulated according to its frequency. In the steps below, functions are defined for how each series will be manipulated based on its frequency. Daily and weekly metrics are resampled to monthly information in a variety of ways. Quarterly and annual metrics are imputed to the monthly level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function defines how daily data is converted to monthly data. After the data is cleaned, a variety of steps are performed:\n",
    "1. The change in the variable over a variety of periods is calculated. For example, the change from the prior day is calculated, as is the change from 365 days prior and several periods in between.\n",
    "2. For each column of data - including the metric and the differences calculated above - the minimum, maximum, mean, median, and standard deviation is calculated for each month represented in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_to_monthly(series, series_name):\n",
    "    \"\"\"Take in a daily dataset, and convert it to a monthly df of statistics\"\"\"\n",
    "    # Create a series of dates containing the BOM and EOM values of interest\n",
    "    start_date = series.index.min()\n",
    "    end_date = series.index.max()\n",
    "    \n",
    "    # Set the start date to the beginning of the earliest month in the data\n",
    "    start_date = dt.date(start_date.year, start_date.month, 1)\n",
    "    \n",
    "    # Set the end date to the end of the latest month in the date\n",
    "    end_date = dt.date(end_date.year, end_date.month, calendar.monthrange(end_date.year, end_date.month)[1])\n",
    "    \n",
    "    # Create a dataframe containing the entire daterange between the start and end dates\n",
    "    df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date))\n",
    "    \n",
    "    # Merge the series into the dataframe of dates\n",
    "    df = df.merge(right=series, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Forward-fill the NA values\n",
    "    df = df.fillna(method='ffill')\n",
    "    \n",
    "    # In case there are any NA vaulues remaining, backfill them\n",
    "    df = df.fillna(method='bfill')\n",
    "    \n",
    "    # Calculate each of the differences between dates\n",
    "    diff_ranges = [1, 2, 3, 4, 5, 6, 7, 10, 14, 21, 28, 180, 365]\n",
    "    for d in diff_ranges:\n",
    "        shifted_df = pd.concat((df.value, df.value.shift(d).rename('shifted_value')), axis=1)\n",
    "        shifted_df.loc[:, f'{d}day_val_ch'] = shifted_df.loc[:, 'value'] - shifted_df.loc[:, 'shifted_value']\n",
    "        shifted_df.loc[:, f'{d}day_per_ch'] = shifted_df.loc[:, f'{d}day_val_ch'] / shifted_df.loc[:, 'value']\n",
    "        shifted_df = shifted_df[[f'{d}day_val_ch', f'{d}day_per_ch']]\n",
    "        df = df.merge(right=shifted_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Create the month grouper for the dataframe\n",
    "    m_grouper = df.groupby(pd.Grouper(freq='M'))\n",
    "    \n",
    "    # Create the result dataframe, which will be a dataframe containing each month and its relevant calculated values.\n",
    "    # Start out by calculating the max values\n",
    "    result_df = m_grouper.max()\n",
    "    result_df.columns = [str(col) + '_max' for col in result_df.columns]\n",
    "\n",
    "    # Calculate and join in min values\n",
    "    calculated_values = m_grouper.min()\n",
    "    calculated_values.columns = [str(col) + '_min' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate and join in mean values\n",
    "    calculated_values = m_grouper.mean()\n",
    "    calculated_values.columns = [str(col) + '_mean' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate and join in median values\n",
    "    calculated_values = m_grouper.median()\n",
    "    calculated_values.columns = [str(col) + '_median' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate and join in standard deviation of values\n",
    "    calculated_values = m_grouper.std()\n",
    "    calculated_values.columns = [str(col) + '_std' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Add the name of the series to each column name\n",
    "    result_df.columns = [series_name + '_' + str(col) for col in result_df.columns]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a similar function is defined to convert weekly data to monthly data. This is performed nearly identically to daily data, involving both calculation of changes from prior weeks and subsequent resampling to monthly granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_to_monthly(series, series_name):\n",
    "    \"\"\"Take in a weekly dataset, and return a monthly df of statistics\"\"\"\n",
    "    # Sort the series by date and turn it into a dataframe\n",
    "    df = series.sort_index()\n",
    "    \n",
    "    # Forward-fill any NA values\n",
    "    df = df.fillna(method='ffill')\n",
    "    \n",
    "    # In case there are any NA vaulues remaining, backfill them\n",
    "    df = df.fillna(method='bfill')\n",
    "    \n",
    "    # Calculate each of the differences between weeks\n",
    "    diff_ranges = [1, 2, 3, 4, 8, 16, 52]\n",
    "    for w in diff_ranges:\n",
    "        shifted_df = pd.concat((df.value, df.value.shift(w).rename('shifted_value')), axis=1)\n",
    "        shifted_df.loc[:, f'{w}wk_val_ch'] = shifted_df.loc[:, 'value'] - shifted_df.loc[:, 'shifted_value']\n",
    "        shifted_df.loc[:, f'{w}wk_per_ch'] = shifted_df.loc[:, f'{w}wk_val_ch'] / shifted_df.loc[:, 'value']\n",
    "        shifted_df = shifted_df[[f'{w}wk_val_ch', f'{w}wk_per_ch']]\n",
    "        df = df.merge(right=shifted_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Create the month grouper for the dataframe\n",
    "    m_grouper = df.groupby(pd.Grouper(freq='M'))\n",
    "    \n",
    "    # Create the result dataframe, which will be a dataframe containing each month and its relevant calculated values.\n",
    "    # Start out by calculating the max values\n",
    "    result_df = m_grouper.max()\n",
    "    result_df.columns = [str(col) + '_max' for col in result_df.columns]\n",
    "\n",
    "    # Calculate and join in min values\n",
    "    calculated_values = m_grouper.min()\n",
    "    calculated_values.columns = [str(col) + '_min' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate and join in mean values\n",
    "    calculated_values = m_grouper.mean()\n",
    "    calculated_values.columns = [str(col) + '_mean' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate and join in median values\n",
    "    calculated_values = m_grouper.median()\n",
    "    calculated_values.columns = [str(col) + '_median' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate and join in standard deviation of values\n",
    "    calculated_values = m_grouper.std()\n",
    "    calculated_values.columns = [str(col) + '_std' for col in calculated_values.columns]\n",
    "    result_df = result_df.merge(right=calculated_values, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Add the name of the series to each column name\n",
    "    result_df.columns = [series_name + '_' + str(col) for col in result_df.columns]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a function defining how to process monthly data is defined. Monthly data requires far fewer calculations, as no resampling is required - only filling missing values is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_monthly(series, series_name):\n",
    "    series.index = [dt.date(i.year, i.month, calendar.monthrange(i.year, i.month)[1]) for i in series.index]\n",
    "    \n",
    "    df = pd.DataFrame(series)\n",
    "    \n",
    "    # Forward-fill the NA values\n",
    "    df = df.fillna(method='ffill')\n",
    "    \n",
    "    # In case there are any NA vaulues remaining, backfill them\n",
    "    df = df.fillna(method='bfill')\n",
    "    \n",
    "    diff_ranges = [1, 2, 3, 6, 9, 12]\n",
    "    for w in diff_ranges:\n",
    "        shifted_df = pd.concat((df.value, df.value.shift(w).rename('shifted_value')), axis=1)\n",
    "        shifted_df.loc[:, f'{w}mth_val_ch'] = shifted_df.loc[:, 'value'] - shifted_df.loc[:, 'shifted_value']\n",
    "        shifted_df.loc[:, f'{w}mth_per_ch'] = shifted_df.loc[:, f'{w}mth_val_ch'] / shifted_df.loc[:, 'value']\n",
    "        shifted_df = shifted_df[[f'{w}mth_val_ch', f'{w}mth_per_ch']]\n",
    "        df = df.merge(right=shifted_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    df.columns = [series_name + '_' + str(col) for col in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions define how quarterly and annual data are converted to monthly data. Both follow the same process - simply using the date of the quarter or year as the value for the month that it takes place, and imputing each missing month. A variety of imputation methods are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarterly_to_monthly(series, series_name):\n",
    "    series.index = [dt.date(i.year, i.month, calendar.monthrange(i.year, i.month)[1]) for i in series.index]\n",
    "    \n",
    "    start_date = series.index.min()\n",
    "    start_date = dt.date(start_date.year, start_date.month, 1)\n",
    "    end_date = series.index.max()\n",
    "\n",
    "    # For quarterly data, the data spans through two months after the latest month shown. Add two months to the end date accordingly.\n",
    "    end_date = end_date + dt.timedelta(days=64)\n",
    "    end_date = dt.date(end_date.year, end_date.month, 1)\n",
    "\n",
    "    # Create a dataframe containing the entire daterange between the start and end dates\n",
    "    df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='M'))\n",
    "\n",
    "    # Merge the series into the dataframe of dates\n",
    "    df = df.merge(right=pd.DataFrame(series), how='left', left_index=True, right_index=True)\n",
    "\n",
    "    df.columns = ['target']\n",
    "    \n",
    "    df[f'interp_linear_{series_name}'] = df.target.interpolate(method='linear')\n",
    "    df[f'interp_quad_{series_name}'] = df.target.interpolate(method='quadratic')\n",
    "    df[f'interp_cubic_{series_name}'] = df.target.interpolate(method='cubic')\n",
    "    df[f'interp_slinear_{series_name}'] = df.target.interpolate(method='slinear')\n",
    "    df[f'interp_akima_{series_name}'] = df.target.interpolate(method='akima')\n",
    "    df[f'interp_poly5_{series_name}'] = df.target.interpolate(method='polynomial', order=5)\n",
    "    df[f'interp_poly7_{series_name}'] = df.target.interpolate(method='polynomial', order=7)\n",
    "\n",
    "    df = df.rename(columns={'target': 'value'})\n",
    "        \n",
    "    diff_ranges = [1, 2, 3, 6, 9, 12]\n",
    "    for w in diff_ranges:\n",
    "        shifted_df = pd.concat((df.value, df.value.shift(w).rename('shifted_value')), axis=1)\n",
    "        shifted_df.loc[:, f'{w}mth_val_ch'] = shifted_df.loc[:, 'value'] - shifted_df.loc[:, 'shifted_value']\n",
    "        shifted_df.loc[:, f'{w}mth_per_ch'] = shifted_df.loc[:, f'{w}mth_val_ch'] / shifted_df.loc[:, 'value']\n",
    "        shifted_df = shifted_df[[f'{w}mth_val_ch', f'{w}mth_per_ch']]\n",
    "        df = df.merge(right=shifted_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Forward-fill the NA values\n",
    "    df = df.fillna(method='ffill')\n",
    "    \n",
    "    # In case there are any NA vaulues remaining, backfill them\n",
    "    df = df.fillna(method='bfill')\n",
    "    \n",
    "    df.columns = [series_name + '_' + str(col) for col in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_to_monthly(series, series_name):\n",
    "    series.index = [dt.date(i.year, i.month, calendar.monthrange(i.year, i.month)[1]) for i in series.index]\n",
    "    \n",
    "    start_date = series.index.min()\n",
    "    start_date = dt.date(start_date.year, start_date.month, 1)\n",
    "    end_date = series.index.max()\n",
    "\n",
    "    # For yearly data, the data spans through eleven months after the latest month shown. Add eleven months to the end date accordingly.\n",
    "    end_date = end_date + dt.timedelta(days=360)\n",
    "    end_date = dt.date(end_date.year, end_date.month, 1)\n",
    "\n",
    "    # Create a dataframe containing the entire daterange between the start and end dates\n",
    "    df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='M'))\n",
    "\n",
    "    # Merge the series into the dataframe of dates\n",
    "    df = df.merge(right=pd.DataFrame(series), how='left', left_index=True, right_index=True)\n",
    "\n",
    "    df.columns = ['target']\n",
    "    \n",
    "    df[f'interp_linear_{series_name}'] = df.target.interpolate(method='linear')\n",
    "    df[f'interp_quad_{series_name}'] = df.target.interpolate(method='quadratic')\n",
    "    df[f'interp_cubic_{series_name}'] = df.target.interpolate(method='cubic')\n",
    "    df[f'interp_slinear_{series_name}'] = df.target.interpolate(method='slinear')\n",
    "    df[f'interp_akima_{series_name}'] = df.target.interpolate(method='akima')\n",
    "    df[f'interp_poly5_{series_name}'] = df.target.interpolate(method='polynomial', order=5)\n",
    "    df[f'interp_poly7_{series_name}'] = df.target.interpolate(method='polynomial', order=7)\n",
    "\n",
    "    df = df.rename(columns={'target': 'value'})\n",
    "\n",
    "    diff_ranges = [1, 2, 3, 6, 9, 12]\n",
    "    for w in diff_ranges:\n",
    "        shifted_df = pd.concat((df.value, df.value.shift(w).rename('shifted_value')), axis=1)\n",
    "        shifted_df.loc[:, f'{w}mth_val_ch'] = shifted_df.loc[:, 'value'] - shifted_df.loc[:, 'shifted_value']\n",
    "        shifted_df.loc[:, f'{w}mth_per_ch'] = shifted_df.loc[:, f'{w}mth_val_ch'] / shifted_df.loc[:, 'value']\n",
    "        shifted_df = shifted_df[[f'{w}mth_val_ch', f'{w}mth_per_ch']]\n",
    "        df = df.merge(right=shifted_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Forward-fill the NA values\n",
    "    df = df.fillna(method='ffill')\n",
    "    \n",
    "    # In case there are any NA vaulues remaining, backfill them\n",
    "    df = df.fillna(method='bfill')\n",
    "    \n",
    "    df.columns = [series_name + '_' + str(col) for col in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the Data\n",
    "\n",
    "Each of the FRED series that was enumerated in the s_list above is pulled using the FRED API. Depending on the data's frequency, it is passed into the appropriate function defined above.\n",
    "\n",
    "Each series is then lagged so that the data would not have been known at the time for which the independent variable is being calculated, combined into one dataframe, and exported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each series is run through the functions defined above according to its frequency. Each function returns a dataframe, and each returned dataframe is put into a dictionary. The dataframes will later be combined into one large dataframe of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting on {'name': 'Ten Year Minus Two Year Yield Diff', 'code': 'T10Y2Y', 'freq': 'd'}...\n",
      "starting on {'name': 'Ten Year Minus Three Month Yield Diff', 'code': 'T10Y3M', 'freq': 'd'}...\n",
      "starting on {'name': 'CPI - All Items', 'code': 'CPIAUCSL', 'freq': 'm'}...\n",
      "starting on {'name': 'Civilian Unemployment Rate', 'code': 'UNRATE', 'freq': 'm'}...\n",
      "starting on {'name': 'US RGDP', 'code': 'GDPC1', 'freq': 'q'}...\n",
      "starting on {'name': '30-Year Fixed Mortgage Average', 'code': 'MORTGAGE30US', 'freq': 'w'}...\n",
      "starting on {'name': 'US NGDP', 'code': 'GDP', 'freq': 'q'}...\n",
      "error: GDP not found\n",
      "starting on {'name': 'Effective Federal Funds Rate', 'code': 'FEDFUNDS', 'freq': 'm'}...\n",
      "starting on {'name': 'Ten Year Treasury Constant Maturity Rate', 'code': 'DGS10', 'freq': 'd'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield Master II Option-Adjusted Spread', 'code': 'BAMLH0A0HYM2', 'freq': 'd'}...\n",
      "starting on {'name': 'S&P/Case-Shiller U.S. National Home Price Index', 'code': 'CSUSHPINSA', 'freq': 'm'}...\n",
      "starting on {'name': '3-Month LIBOR', 'code': 'USD3MTD156N', 'freq': 'd'}...\n",
      "starting on {'name': '3-Month Treasury Bill Secondary Rate', 'code': 'TB3MS', 'freq': 'm'}...\n",
      "starting on {'name': '1-Month LIBOR', 'code': 'USD1MTD156N', 'freq': 'd'}...\n",
      "starting on {'name': 'M2 Money Stock', 'code': 'M2', 'freq': 'w'}...\n",
      "starting on {'name': '10-Year Breakeven Inflation Rate', 'code': 'T10YIE', 'freq': 'd'}...\n",
      "starting on {'name': 'Industrial Production Index', 'code': 'INDPRO', 'freq': 'm'}...\n",
      "starting on {'name': 'TED Spread', 'code': 'TEDRATE', 'freq': 'd'}...\n",
      "starting on {'name': 'Trade-Weighted US Dollar Index', 'code': 'TWEXB', 'freq': 'w'}...\n",
      "starting on {'name': 'Federal Debt: Total Public Debt as Percent of GDP', 'code': 'GFDEGDQ188S', 'freq': 'q'}...\n",
      "starting on {'name': '1-Year Treasury Constant Maturity Rate', 'code': 'DGS1', 'freq': 'd'}...\n",
      "starting on {'name': 'All Employees: Total Nonfarm Payrolls', 'code': 'PAYEMS', 'freq': 'm'}...\n",
      "starting on {'name': 'Smoothed U.S. Recession Probabilities', 'code': 'RECPROUSM156N', 'freq': 'm'}...\n",
      "starting on {'name': 'Leading Index for the United States', 'code': 'USSLIND', 'freq': 'm'}...\n",
      "starting on {'name': 'S&P 500', 'code': 'SP500', 'freq': 'd'}...\n",
      "starting on {'name': 'Real Median Household Income in the United States', 'code': 'MEHOINUSA672N', 'freq': 'a'}...\n",
      "starting on {'name': \"Moody's Seasoned Baa Corporate Bond Yield\", 'code': 'BAA', 'freq': 'm'}...\n",
      "starting on {'name': \"Moody's Seasoned Aaa Corporate Bond Yield\", 'code': 'AAA', 'freq': 'm'}...\n",
      "starting on {'name': 'St. Louis Fed Financial Stress Index', 'code': 'STLFSI', 'freq': 'w'}...\n",
      "starting on {'name': 'ICE BofAML US Corporate BBB Option-Adjusted Spread', 'code': 'BAMLC0A4CBBB', 'freq': 'd'}...\n",
      "starting on {'name': 'Crude Oil Prices: West Texas Intermediate (WTI) - Cushing, Oklahoma', 'code': 'DCOILWTICO', 'freq': 'd'}...\n",
      "starting on {'name': '5-Year, 5-Year Forward Inflation Expectation Rate', 'code': 'T5YIFR', 'freq': 'd'}...\n",
      "starting on {'name': '2-Year Treasury Constant Maturity Rate', 'code': 'DGS2', 'freq': 'd'}...\n",
      "starting on {'name': 'Housing Starts: Total: New Privately Owned Housing Units Started', 'code': 'HOUST', 'freq': 'm'}...\n",
      "starting on {'name': 'U.S. / Euro Foreign Exchange Rate', 'code': 'DEXUSEU', 'freq': 'd'}...\n",
      "starting on {'name': 'S&P/Case-Shiller 20-City Composite Home Price Index', 'code': 'SPCS20RSA', 'freq': 'm'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield Master II Effective Yield', 'code': 'BAMLH0A0HYM2EY', 'freq': 'd'}...\n",
      "starting on {'name': '30-Year Treasury Constant Maturity Rate', 'code': 'DGS30', 'freq': 'd'}...\n",
      "starting on {'name': \"Moody's Seasoned Baa Corporate Bond Yield Relative to Yield on 10-Year Treasury Constant Maturity\", 'code': 'BAA10Y', 'freq': 'd'}...\n",
      "starting on {'name': 'Personal Saving Rate', 'code': 'PSAVERT', 'freq': 'm'}...\n",
      "starting on {'name': '5-Year Treasury Constant Maturity Rate', 'code': 'DGS5', 'freq': 'd'}...\n",
      "starting on {'name': 'Gold Fixing Price 10:30 A.M. (London time) in London Bullion Market, based in U.S. Dollars', 'code': 'GOLDAMGBD228NLBM', 'freq': 'd'}...\n",
      "starting on {'name': '10-Year Treasury Constant Maturity Minus Federal Funds Rate', 'code': 'T10YFF', 'freq': 'd'}...\n",
      "starting on {'name': 'Federal Surplus or Deficit', 'code': 'FYFSD', 'freq': 'a'}...\n",
      "starting on {'name': 'Median Sales Price of Houses Sold for the United States', 'code': 'MSPUS', 'freq': 'q'}...\n",
      "starting on {'name': 'Producer Price Index by Commodity for Pulp, Paper, and Allied Products: Wood Pulp', 'code': 'WPU0911', 'freq': 'q'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield CCC or Below Option-Adjusted Spread', 'code': 'BAMLH0A3HYC', 'freq': 'd'}...\n",
      "starting on {'name': 'Total Vehicle Sales', 'code': 'TOTALSA', 'freq': 'm'}...\n",
      "starting on {'name': 'Initial Claims', 'code': 'ICSA', 'freq': 'w'}...\n",
      "starting on {'name': 'Interest Rates, Discount Rate for United States', 'code': 'INTDSRUSM193N', 'freq': 'm'}...\n",
      "starting on {'name': 'Civilian Labor Force Participation Rate', 'code': 'CIVPART', 'freq': 'm'}...\n",
      "starting on {'name': 'Dow Jones Industrial Average', 'code': 'DJIA', 'freq': 'd'}...\n",
      "starting on {'name': 'Real gross domestic product per capita', 'code': 'A939RX0Q048SBEA', 'freq': 'q'}...\n",
      "starting on {'name': 'Delinquency Rate on Single-Family Residential Mortgages, Booked in Domestic Offices, All Commercial Banks', 'code': 'DRSFRMACBN', 'freq': 'q'}...\n",
      "starting on {'name': 'Real Disposable Personal Income', 'code': 'DSPIC96', 'freq': 'm'}...\n",
      "starting on {'name': 'Monthly Supply of Houses in the United States', 'code': 'MSACSR', 'freq': 'm'}...\n",
      "starting on {'name': 'Employed full time: Median usual weekly real earnings: Wage and salary workers: 16 years and over', 'code': 'LES1252881600Q', 'freq': 'q'}...\n",
      "starting on {'name': 'All Employees: Manufacturing', 'code': 'MANEMP', 'freq': 'm'}...\n",
      "starting on {'name': 'Corporate Profits After Tax (without IVA and CCAdj)', 'code': 'CP', 'freq': 'q'}...\n",
      "starting on {'name': 'ICE BofAML US Corporate Master Option-Adjusted Spread', 'code': 'BAMLC0A0CM', 'freq': 'd'}...\n",
      "starting on {'name': '3-Month Treasury Constant Maturity Rate', 'code': 'DGS3MO', 'freq': 'd'}...\n",
      "starting on {'name': 'Unemployment Rate: 20 years and over', 'code': 'LNS14000024', 'freq': 'm'}...\n",
      "starting on {'name': 'All-Transactions House Price Index for the United States', 'code': 'USSTHPI', 'freq': 'q'}...\n",
      "starting on {'name': '15-Year Fixed Rate Mortgage Average in the United States', 'code': 'MORTGAGE15US', 'freq': 'w'}...\n",
      "starting on {'name': 'Commercial and Industrial Loans, All Commercial Banks', 'code': 'BUSLOANS', 'freq': 'm'}...\n",
      "starting on {'name': 'Stock Market Capitalization to GDP for United States', 'code': 'DDDM01USA156NWDB', 'freq': 'a'}...\n",
      "starting on {'name': 'Household Debt Service Payments as a Percent of Disposable Personal Income', 'code': 'TDSP', 'freq': 'q'}...\n",
      "starting on {'name': 'Bank Prime Loan Rate', 'code': 'MPRIME', 'freq': 'm'}...\n",
      "starting on {'name': 'CBOE Volatility Index: VIX', 'code': 'VIXCLS', 'freq': 'd'}...\n",
      "starting on {'name': 'NBER based Recession Indicators for the United States from the Period following the Peak through the Trough', 'code': 'USREC', 'freq': 'm'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield Master II Total Return Index Value', 'code': 'BAMLHYH0A0HYM2TRIV', 'freq': 'd'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield CCC or Below Effective Yield', 'code': 'BAMLH0A3HYCEY', 'freq': 'd'}...\n",
      "starting on {'name': '10-Year High Quality Market (HQM) Corporate Bond Spot Rate', 'code': 'HQMCB10YR', 'freq': 'm'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield BB Option-Adjusted Spread', 'code': 'BAMLH0A1HYBB', 'freq': 'd'}...\n",
      "starting on {'name': '12-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar', 'code': 'USD12MD156N', 'freq': 'd'}...\n",
      "starting on {'name': 'Gross Private Domestic Investment', 'code': 'GPDI', 'freq': 'q'}...\n",
      "starting on {'name': 'Real Gross Private Domestic Investment', 'code': 'GPDIC1', 'freq': 'q'}...\n",
      "starting on {'name': \"Manufacturers' New Orders: Durable Goods\", 'code': 'DGORDER', 'freq': 'm'}...\n",
      "starting on {'name': 'Producer Price Index for All Commodities', 'code': 'PPIACO', 'freq': 'm'}...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting on {'name': 'Chicago Fed National Financial Conditions Index', 'code': 'NFCI', 'freq': 'w'}...\n",
      "starting on {'name': 'Unemployment Rate: 20 years and over, Black or African American Men', 'code': 'LNS14000031', 'freq': 'm'}...\n",
      "starting on {'name': 'Nonfinancial corporate business; debt securities; liability, Level', 'code': 'NCBDBIQ027S', 'freq': 'q'}...\n",
      "starting on {'name': 'Capacity Utilization: Total Industry', 'code': 'TCU', 'freq': 'm'}...\n",
      "starting on {'name': 'ICE BofAML US High Yield BB Effective Yield', 'code': 'BAMLH0A1HYBBEY', 'freq': 'd'}...\n",
      "starting on {'name': 'Average Sales Price of Houses Sold for the United States', 'code': 'ASPUS', 'freq': 'q'}...\n",
      "starting on {'name': 'Real Disposable Personal Income: Per Capita', 'code': 'A229RX0', 'freq': 'm'}...\n",
      "starting on {'name': 'Unemployment Level', 'code': 'UNEMPLOY', 'freq': 'm'}...\n",
      "starting on {'name': 'Homeownership Rate for the United States', 'code': 'RHORUSQ156N', 'freq': 'q'}...\n",
      "starting on {'name': 'Motor Vehicle Retail Sales: Heavy Weight Trucks', 'code': 'HTRUCKSSAAR', 'freq': 'm'}...\n",
      "starting on {'name': 'Civilian Labor Force Participation Rate: 25 to 54 years', 'code': 'LNS11300060', 'freq': 'm'}...\n",
      "starting on {'name': 'University of Michigan: Consumer Sentiment', 'code': 'UMCSENT', 'freq': 'm'}...\n",
      "starting on {'name': 'Consumer Opinion Surveys: Confidence Indicators: Composite Indicators: OECD Indicator for the United States', 'code': 'CSCICP03USM665S', 'freq': 'm'}...\n",
      "starting on {'name': 'Business Tendency Surveys for Manufacturing: Confidence Indicators: Composite Indicators: OECD Indicator for the United States', 'code': 'BSCICP03USM665S', 'freq': 'm'}...\n",
      "starting on {'name': 'Sahm Real-Time Recession Indicator', 'code': 'SAHMREALTIME', 'freq': 'm'}...\n"
     ]
    }
   ],
   "source": [
    "df_dict = {}\n",
    "for s in s_list:\n",
    "    print(f'starting on {s}...')\n",
    "    # Occasionally, there is a communication error with the server that the requests are made to. The code below is set up to   \n",
    "    j = 0\n",
    "    while j < 5:\n",
    "        try:\n",
    "            current_series = fred.get_series(s['code']).to_frame()\n",
    "            current_series.columns = ['value']\n",
    "            if s['freq'] == 'd':\n",
    "                df_dict[s['code']] = daily_to_monthly(current_series, s['code'])\n",
    "            elif s['freq'] == 'w':\n",
    "                df_dict[s['code']] = weekly_to_monthly(current_series, s['code'])\n",
    "            elif s['freq'] == 'm':\n",
    "                df_dict[s['code']] = process_monthly(current_series, s['code'])\n",
    "            elif s['freq'] == 'q':\n",
    "                df_dict[s['code']] = quarterly_to_monthly(current_series, s['code'])\n",
    "            elif s['freq'] == 'a':\n",
    "                df_dict[s['code']] = annual_to_monthly(current_series, s['code'])\n",
    "        except:\n",
    "            print(f\"error: {s['code']} not found\")\n",
    "            j += 1\n",
    "            time.sleep(10)\n",
    "            assert j < 5, \"5 iterations tried on {s['code']}\"\n",
    "            continue\n",
    "        j = 5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the dependent variable - the default rate of credit card loans - is pulled and resampled to monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_series = fred.get_series('DRCCLACBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dependent series index to the last day in the month to correspond to the rest of the datasets\n",
    "dep_series.index = [dt.date(i.year, i.month, calendar.monthrange(i.year, i.month)[1]) for i in dep_series.index]\n",
    "\n",
    "# Determine the start and end dates for the series, which will be used later on.\n",
    "start_date = dep_series.index.min()\n",
    "start_date = dt.date(start_date.year, start_date.month, 1)\n",
    "end_date = dep_series.index.max()\n",
    "\n",
    "# For quarterly data, the data spans through two months after the latest month shown. Add two months to the end date accordingly.\n",
    "end_date = end_date + dt.timedelta(days=64)\n",
    "end_date = dt.date(end_date.year, end_date.month, 1)\n",
    "\n",
    "# Create a dataframe containing the entire daterange between the start and end dates\n",
    "dep_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='M'))\n",
    "\n",
    "# Merge the series into the dataframe of dates\n",
    "dep_df = dep_df.merge(right=pd.DataFrame(dep_series), how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Create a copy of the dependent data pre-imputation. This will be used later on in the project.\n",
    "dep_df_raw = dep_df.copy()\n",
    "\n",
    "# Interpolate the data using the quadratic method, which seems to best fit the expected path of defaults\n",
    "dep_df.columns = ['target']\n",
    "dep_df[f'interp_quad'] = dep_df.target.interpolate(method='quadratic')\n",
    "dep_df = dep_df.ffill()\n",
    "\n",
    "dep_df = dep_df[['interp_quad']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lags for each variable in the dataframe are now added, as it is possible that variables from prior months have an influence on the dependent variable. For example, each row is modified to contain both the variable values from the month that the row represents, and the variable values from the prior month.\n",
    "\n",
    "The dataframes are then combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df = pd.DataFrame(index=dep_df.index)\n",
    "\n",
    "for k, series_df in df_dict.items():\n",
    "    ind_df = pd.merge(left=ind_df, right=series_df, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the independent and dependent data are exported to pickle files for future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df.to_pickle(r'data\\ind_df.pkl')\n",
    "dep_df.to_pickle(r'data\\dep_df.pkl')\n",
    "dep_df_raw.to_pickle(r'data\\dep_df_raw.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
